{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tekle-eyesus/nlp-basics-assignment-/blob/main/NLP_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TStWu4cV9Rdz"
      },
      "source": [
        "# Natural Language Processing (NLP) Assignment\n",
        "This assignment will guide you through the basic concepts of Natural Language Processing including:\n",
        "- Text preprocessing\n",
        "- Tokenization and N-grams\n",
        "- Named Entity Recognition (NER)\n",
        "- Converting text into numbers (vectorization)\n",
        "- Word embeddings (for experienced learners)\n",
        "\n",
        "You can run and modify the code cells below to complete the tasks."
      ],
      "id": "TStWu4cV9Rdz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaK6MYJL9Rd8",
        "outputId": "7b501b88-4d9f-4826-ad34-cee5fa7fb1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "XaK6MYJL9Rd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D2Yvlil9ReA"
      },
      "source": [
        "## 1. Text Preprocessing\n",
        "Clean the following text by converting it to lowercase, removing punctuation and stop words."
      ],
      "id": "7D2Yvlil9ReA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc0DWsjk9ReC",
        "outputId": "352aaec2-3689-4abf-97d4-eb14c36904cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'fascinating', 'field', 'combines', 'linguistics', 'computer', 'science']\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "text = \"Natural Language Processing is a fascinating field. It combines linguistics and computer science!\"\n",
        "\n",
        "# TODO: Preprocess the text\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove punctuation and stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Print cleaned tokens\n",
        "cleaned_tokens = preprocess(text)\n",
        "print(cleaned_tokens)"
      ],
      "id": "Yc0DWsjk9ReC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uho4Ui1V9ReD"
      },
      "source": [
        "## 2. Tokenization and N-grams\n",
        "Generate bigrams (2-grams) from the cleaned tokens."
      ],
      "id": "Uho4Ui1V9ReD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCsbjnV19ReE",
        "outputId": "07a55ae7-d567-4545-c700-d1780670bff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'fascinating'), ('fascinating', 'field'), ('field', 'combines'), ('combines', 'linguistics'), ('linguistics', 'computer'), ('computer', 'science')]\n"
          ]
        }
      ],
      "source": [
        "# Generate bigrams from cleaned tokens\n",
        "bigrams = list(ngrams(cleaned_tokens, 2))\n",
        "print(\"Bigrams:\", bigrams)"
      ],
      "id": "FCsbjnV19ReE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsnE2ktL9ReG"
      },
      "source": [
        "## 3. Named Entity Recognition (NER)\n",
        "Use spaCy to perform NER on a new sentence."
      ],
      "id": "tsnE2ktL9ReG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4cSHHvN9ReH",
        "outputId": "28d8ad42-40ce-45b9-e001-4cac42273208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack Obama PERSON\n",
            "Hawaii GPE\n",
            "2008 DATE\n",
            "Natural Language Processing ORG\n"
          ]
        }
      ],
      "source": [
        "# Example sentence\n",
        "sentence = \"Barack Obama was born in Hawaii and was elected president in 2008.\"\n",
        "doc = nlp(sentence)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "# for the givven text\n",
        "\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "id": "z4cSHHvN9ReH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqbstKd89ReI"
      },
      "source": [
        "## 4. Converting Text to Numbers\n",
        "Use CountVectorizer and TfidfVectorizer to convert a list of sentences into numeric vectors."
      ],
      "id": "jqbstKd89ReI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc0emnIt9ReJ",
        "outputId": "941968af-9346-4251-c446-ecadee64b30e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectorizer Output:\n",
            " [[0 0 0 0 1 1 1 0 0 0 0 0]\n",
            " [1 0 1 1 0 0 0 1 1 1 1 0]\n",
            " [1 1 1 0 0 0 0 0 0 0 0 1]]\n",
            "\n",
            "TF-IDF Vectorizer Output:\n",
            " [[0.         0.         0.         0.         0.57735027 0.57735027\n",
            "  0.57735027 0.         0.         0.         0.         0.        ]\n",
            " [0.30650422 0.         0.30650422 0.40301621 0.         0.\n",
            "  0.         0.40301621 0.40301621 0.40301621 0.40301621 0.        ]\n",
            " [0.42804604 0.5628291  0.42804604 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.5628291 ]]\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Natural language processing is a part of AI.\",\n",
        "    \"AI is the future.\"\n",
        "]\n",
        "\n",
        "# CountVectorizer\n",
        "count_vec = CountVectorizer()\n",
        "X_count = count_vec.fit_transform(sentences)\n",
        "print(\"Count Vectorizer Output:\\n\", X_count.toarray())\n",
        "\n",
        "# TfidfVectorizer\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vec.fit_transform(sentences)\n",
        "print(\"\\nTF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())"
      ],
      "id": "jc0emnIt9ReJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_NMNcHK9ReK"
      },
      "source": [
        "## 5. Word Embeddings (Advanced)\n",
        "Use spaCy to get word vectors (embeddings) for given words."
      ],
      "id": "W_NMNcHK9ReK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xie9-Bhj9ReL",
        "outputId": "c7a35ce1-b626-4634-bfde-6ae976433660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine':\n",
            " [-0.72883    0.20718   -0.0033379 -0.0027673 -0.17204    0.023277\n",
            "  0.1297    -0.2112     0.32876    0.67447    0.10047   -0.30559\n",
            "  0.11213    0.22959   -0.32997    0.1389    -0.57289    2.523\n",
            " -0.32921    0.06045    0.23895    0.1091     0.19358   -0.1765\n",
            "  0.11583    0.63204   -0.13644   -0.24354    0.20061   -0.50244\n",
            "  0.40537   -0.38688    0.73784    0.093937  -0.30643    0.045874\n",
            "  0.097915  -0.082114   0.13082   -0.039022   0.088084  -0.27023\n",
            " -0.077658  -0.0045355  0.18986   -0.063083  -0.138      0.40474\n",
            " -0.16199   -0.10953    0.22923   -0.67634   -0.65763   -0.044595\n",
            " -0.12119    0.071167   0.25993   -0.27052   -0.22474   -0.13818\n",
            "  0.20692    0.87604   -0.35257   -0.1498     0.72804    0.68768\n",
            "  0.19993    0.084733  -0.2234     0.11301    0.29895   -0.090119\n",
            "  0.038172  -0.32912    0.014221  -0.36335    0.5898     0.10467\n",
            "  0.16549    0.47199    0.078939  -0.19985    0.84014   -0.2277\n",
            " -0.22907   -0.26243   -0.32598    1.0146    -0.079235  -0.34248\n",
            "  0.032767   0.49757    0.0047373  0.057762   0.19319    0.10756\n",
            "  0.16938    0.42513   -0.22691    0.095343  -0.094303  -0.3849\n",
            " -0.27853   -0.4554     0.2735    -2.1344     0.040352   0.065232\n",
            "  0.23147    0.13766    0.41638    0.1153     0.61593   -0.012896\n",
            "  0.22778    0.13342    0.12902   -0.0054822 -0.55536    0.56218\n",
            " -0.082096  -0.46214    0.21512   -0.09482   -0.16694   -0.94924\n",
            "  0.08068    0.64041    0.0089012  0.043188  -0.030859   0.02754\n",
            "  0.12671   -0.2175     0.0098018  0.14784    0.37847   -0.32479\n",
            " -0.21623    0.14108    0.069705  -0.18381    0.10896   -0.12666\n",
            " -0.063817  -0.44837   -0.079542  -0.54454    0.32602   -0.089619\n",
            " -0.069878   0.010953  -0.16017   -0.13361    0.37901    0.74596\n",
            " -0.01211   -0.4469     0.16831    0.12325   -0.29108   -0.6984\n",
            " -0.33013   -0.20759   -0.070089   0.023017  -0.32895   -0.02034\n",
            "  0.50521   -0.12432    0.26341   -0.063996  -0.46205   -0.39595\n",
            " -0.15154   -0.22158   -0.050627  -0.015164  -0.38784    0.5011\n",
            "  0.19628   -0.31646   -0.48555   -0.49464    0.35255   -0.060035\n",
            "  0.082212   0.084107   0.17729   -0.55179    0.071874  -0.39032\n",
            "  0.40137   -0.2273     0.35788    0.42503   -0.20496    0.58632\n",
            " -0.2015     0.35892    0.045149  -0.20252    0.15502   -0.019122\n",
            " -0.11768   -0.48471    0.35088    0.14332    0.091038   0.28448\n",
            "  0.35166   -0.87305    0.047971   0.43431   -0.34814    0.035735\n",
            " -0.21673    0.062818  -0.40837   -0.21775    0.1597     0.56172\n",
            " -0.11126    0.33851   -0.31825   -0.10671    0.057792  -0.03997\n",
            " -0.15047   -0.11435    0.56779    0.43056   -0.17674   -0.045657\n",
            "  0.04453   -0.11629    0.094277  -0.46008   -0.12373   -0.18918\n",
            "  0.14565   -0.17643    0.45186   -0.011373   0.16214   -0.17391\n",
            " -0.14195    0.14683   -0.055814   0.52315    0.0032239 -0.51676\n",
            " -0.094159   0.21092    0.22586   -0.78028   -0.67057   -0.031255\n",
            "  0.045657   0.033926   0.16709    0.014854  -0.14456   -0.56059\n",
            " -0.26709   -0.17691    0.15472    0.46348    1.4026    -0.24662\n",
            "  0.3447    -0.56387    0.047655  -0.39049   -0.016389   0.52322\n",
            " -0.22908   -0.31134    0.6579    -0.67904    0.40002    0.055284\n",
            "  0.5956     0.031032  -0.19315   -0.65318   -0.28911    0.13658\n",
            " -0.42426   -0.37742   -0.17644   -0.15885    0.48907    1.0195\n",
            "  0.12087   -0.36731    0.0087637  0.10666   -0.12636    0.66767  ]\n"
          ]
        }
      ],
      "source": [
        "# Note: en_core_web_sm does not have word vectors. You can install and use en_core_web_md\n",
        "# Uncomment below to install and load the medium model if needed.\n",
        "\n",
        "# !python -m spacy download en_core_web_md\n",
        "# nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Example word vector\n",
        "word = nlp(\"machine\")[0]\n",
        "print(\"Vector for 'machine':\\n\", word.vector)"
      ],
      "id": "Xie9-Bhj9ReL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words with similar contexts (like \"machine\" and \"robot\") should have higher similarity scores than unrelated words (like \"machine\" and \"apple\")."
      ],
      "metadata": {
        "id": "NJ8woy2TNGLs"
      },
      "id": "NJ8woy2TNGLs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare word similarities\n",
        "word1 = nlp(\"machine\")[0]\n",
        "word2 = nlp(\"robot\")[0]\n",
        "word3 = nlp(\"apple\")[0]\n",
        "\n",
        "print(f\"Similarity between 'machine' and 'robot': {word1.similarity(word2):.4f}\")\n",
        "print(f\"Similarity between 'machine' and 'apple': {word1.similarity(word3):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ruXX31YM74h",
        "outputId": "c43b6b58-e642-4e8b-b5a2-654046a463e7"
      },
      "id": "2ruXX31YM74h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'machine' and 'robot': 0.4400\n",
            "Similarity between 'machine' and 'apple': 0.0060\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}